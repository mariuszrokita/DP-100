{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:09:15.240485Z",
     "start_time": "2019-12-20T18:09:07.969746Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "#create a folder for the dataset\n",
    "os.makedirs('./data/mnist', exist_ok = True)\n",
    "\n",
    "# load dataset to the directory--as you can see, you must load train sets and test sets separately\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename='./data/mnist/train-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename='./data/mnist/train-labels.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename='./data/mnist/test-images.gz')\n",
    "urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename='./data/mnist/test-labels.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split out the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:13:18.977332Z",
     "start_time": "2019-12-20T18:13:18.968330Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "# load compressed MNIST gz files and return numpy arrays\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res\n",
    "\n",
    "print('Functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:13:21.004224Z",
     "start_time": "2019-12-20T18:13:20.605221Z"
    }
   },
   "outputs": [],
   "source": [
    "# To help the model converge faster, shrink the intensity values (X) from 0-255 to 0-1\n",
    "\n",
    "X_train = load_data('./data/mnist/train-images.gz', False) / 255.0\n",
    "y_train = load_data('./data/mnist/train-labels.gz', True).reshape(-1)\n",
    "\n",
    "X_test = load_data('./data/mnist/test-images.gz', False) / 255.0\n",
    "y_test = load_data('./data/mnist/test-labels.gz', True).reshape(-1)\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:13:24.164576Z",
     "start_time": "2019-12-20T18:13:24.159577Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:13:27.198300Z",
     "start_time": "2019-12-20T18:13:26.055602Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "## Local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T16:31:24.274229Z",
     "start_time": "2019-12-20T16:31:07.612180Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#load the model\n",
    "clf = LogisticRegression()\n",
    "#fit the model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#evaluate the model by using a test set\n",
    "y_hat = clf.predict(X_test)\n",
    "#print the accuracy\n",
    "print(np.average(y_hat == y_test))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Experimentation Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T18:13:30.972860Z",
     "start_time": "2019-12-20T18:13:30.967859Z"
    }
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-20T18:13:32.732Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "subscription_id = ''\n",
    "resource_group = ''\n",
    "region = 'westeurope'\n",
    "\n",
    "if os.path.exists(os.path.join(\".\", \".azureml\", \"config.json\")):\n",
    "    ws = Workspace.from_config()\n",
    "else:\n",
    "    ws = Workspace.create(name='AMLSLearnworkspace',\n",
    "                          subscription_id=subscription_id, \n",
    "                          resource_group=resource_group,\n",
    "                          create_resource_group = True,\n",
    "                          location=region)\n",
    "    \n",
    "    print('AMLS Workspace created')    \n",
    "    # Create the configuration file.\n",
    "    ws.write_config()\n",
    "    print('Configuration saved')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T16:39:53.064319Z",
     "start_time": "2019-12-20T16:39:53.058320Z"
    }
   },
   "outputs": [],
   "source": [
    "# View workspace details\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T16:39:58.084414Z",
     "start_time": "2019-12-20T16:39:57.729460Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "#Create an experiment\n",
    "experiment = Experiment(workspace = ws, name = \"amls-learn-experimentnew5\")\n",
    "\n",
    "print('Experiment created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a remote compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:21:26.852722Z",
     "start_time": "2019-12-20T17:21:26.163460Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# Step 1: name the cluster and set the minimal and maximal number of nodes \n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\n",
    "max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 3)\n",
    "idle_seconds_before_scaledown = 600\n",
    "\n",
    "# Step 2: choose environment variables \n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, \n",
    "                                                            min_nodes = min_nodes, \n",
    "                                                            max_nodes = max_nodes, \n",
    "                                                            idle_seconds_before_scaledown = idle_seconds_before_scaledown)\n",
    "\n",
    "# create the cluster\n",
    "compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "print('Compute target created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to AMLS's data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:45:42.954621Z",
     "start_time": "2019-12-20T17:45:28.787698Z"
    }
   },
   "outputs": [],
   "source": [
    "#upload data by using get_default_datastore()\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path='data', overwrite=True, show_progress=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a modeling script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:22:23.832105Z",
     "start_time": "2019-12-20T17:22:23.826103Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# create the folder\n",
    "folder_training_script = './trial_model_mnist'\n",
    "os.makedirs(folder_training_script, exist_ok=True)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:25:29.360352Z",
     "start_time": "2019-12-20T17:25:29.353351Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile $folder_training_script/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from azureml.core import Run\n",
    "# from utils import load_data\n",
    "\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "# load compressed MNIST gz files and return numpy arrays\n",
    "def load_data(filename, label=False):\n",
    "    with gzip.open(filename) as gz:\n",
    "        struct.unpack('I', gz.read(4))\n",
    "        n_items = struct.unpack('>I', gz.read(4))\n",
    "        if not label:\n",
    "            n_rows = struct.unpack('>I', gz.read(4))[0]\n",
    "            n_cols = struct.unpack('>I', gz.read(4))[0]\n",
    "            res = np.frombuffer(gz.read(n_items[0] * n_rows * n_cols), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], n_rows * n_cols)\n",
    "        else:\n",
    "            res = np.frombuffer(gz.read(n_items[0]), dtype=np.uint8)\n",
    "            res = res.reshape(n_items[0], 1)\n",
    "    return res\n",
    "\n",
    "\n",
    "# let user feed in 2 parameters, the dataset to mount or download, \n",
    "# and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "parser.add_argument('--local', type=bool, dest='local', default=False, \n",
    "                               help='Flag indicating where model training takes place')\n",
    "args = parser.parse_args()\n",
    "\n",
    "###\n",
    "data_folder = os.path.join(args.data_folder, 'data', 'mnist')\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# load the train and test set into numpy arrays\n",
    "X_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\n",
    "X_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\n",
    "\n",
    "#print variable set dimension\n",
    "print(X_train.shape, X_test.shape, sep = '\\n')\n",
    "\n",
    "y_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\n",
    "y_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n",
    "\n",
    "#print the response variable dimension\n",
    "print( y_train.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "print('Train a logistic regression model with regularization rate of', args.reg)\n",
    "clf = LogisticRegression(C=1.0/args.reg, solver=\"liblinear\", multi_class=\"auto\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Predict the test set')\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy on the prediction\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy is', acc)\n",
    "\n",
    "run.log('regularization rate', np.float(args.reg))\n",
    "run.log('accuracy', np.float(acc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')\n",
    "\n",
    "# register an Azure ML model (it's only possible when training in the AMLS)\n",
    "if args.local == False:\n",
    "    model = run.register_model(model_name='sklearn_mnist_model.pkl',\n",
    "                               model_path='outputs/sklearn_mnist_model.pkl',\n",
    "                               tags = {'area': \"MNIST\", 'type': \"sklearn\"},\n",
    "                               description = \"identify numbers\")\n",
    "\n",
    "    print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the modeling script locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: By running the script locally we are making sure that we can also run the script successfully on the remote compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:27:15.389468Z",
     "start_time": "2019-12-20T17:25:31.769042Z"
    }
   },
   "outputs": [],
   "source": [
    "! python ./trial_model_mnist/train.py --data-folder=. --regularization=0.6 --local=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:27:20.101561Z",
     "start_time": "2019-12-20T17:27:20.057429Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a configuration for the run\n",
    "\n",
    "from azureml.train.sklearn import SKLearn\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.as_mount(),\n",
    "    '--regularization': 0.5,\n",
    "    '--local': False\n",
    "}\n",
    "\n",
    "#import the Scikit-learn package \n",
    "est = SKLearn(source_directory=folder_training_script,\n",
    "                script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:27:42.109280Z",
     "start_time": "2019-12-20T17:27:23.716905Z"
    }
   },
   "outputs": [],
   "source": [
    "run = experiment.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T17:38:51.960974Z",
     "start_time": "2019-12-20T17:38:51.061978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the result\n",
    "print(run.get_metrics())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
