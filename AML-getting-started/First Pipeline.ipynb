{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pipeline is based on the tutorial available at: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-create-your-first-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ML pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:37:29.018108Z",
     "start_time": "2019-12-31T14:37:27.664099Z"
    }
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:37:35.972564Z",
     "start_time": "2019-12-31T14:37:34.288699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Default datastore \n",
    "def_data_store = ws.get_default_datastore()\n",
    "\n",
    "# Get the blob storage associated with the workspace\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "\n",
    "# Get file storage associated with the workspace\n",
    "def_file_store = Datastore(ws, \"workspacefilestore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data files to datatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:37:40.242269Z",
     "start_time": "2019-12-31T14:37:37.960351Z"
    }
   },
   "outputs": [],
   "source": [
    "# The data file downloaded from:\n",
    "# https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/20news.pkl\n",
    "\n",
    "def_blob_store.upload_files(\n",
    "    [\"./data/20newsgroups/20news.pkl\"],\n",
    "    target_path=\"20newsgroups\",\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up compute target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:07.301499Z",
     "start_time": "2019-12-31T14:37:44.871099Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "compute_name = \"aml-compute\"\n",
    "vm_size = \"STANDARD_D2_V2\"\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target: ' + compute_name)\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n",
    "                                                                min_nodes=0,\n",
    "                                                                max_nodes=4,\n",
    "                                                                idle_seconds_before_scaledown=600)\n",
    "    # create the compute target\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current cluster status, use the 'status' property\n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:10.636481Z",
     "start_time": "2019-12-31T14:38:10.495236Z"
    }
   },
   "outputs": [],
   "source": [
    "# list of Compute Targets on the workspace\n",
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:12.275850Z",
     "start_time": "2019-12-31T14:38:12.271847Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:13.947910Z",
     "start_time": "2019-12-31T14:38:13.943909Z"
    }
   },
   "outputs": [],
   "source": [
    "blob_input_data = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"test_data\",\n",
    "    path_on_datastore=\"20newsgroups/20news.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermediate/Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:15.546562Z",
     "start_time": "2019-12-31T14:38:15.543587Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_data1 = PipelineData(\"processed_data1\", datastore=def_blob_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:16.595261Z",
     "start_time": "2019-12-31T14:38:16.591260Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_data2 = PipelineData(\"processed_data2\", datastore=def_blob_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:17.740404Z",
     "start_time": "2019-12-31T14:38:17.735381Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_data3 = PipelineData(\"processed_data3\", datastore=def_blob_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T11:37:54.713188Z",
     "start_time": "2019-12-31T11:37:54.709154Z"
    }
   },
   "source": [
    "## Construct pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the `source_directory` for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the `source_directory` would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the `source_directory` of the step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:20.689350Z",
     "start_time": "2019-12-31T14:38:20.661883Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "source_directory = 'data_dependency_run_train'\n",
    "print(f\"Source directory for the step is {os.path.realpath(source_directory)}.\")\n",
    "\n",
    "trainStep = PythonScriptStep(\n",
    "    script_name=\"train.py\",\n",
    "    arguments=[\"--input\", blob_input_data, \"--output\", processed_data1],\n",
    "    inputs=[blob_input_data],\n",
    "    outputs=[processed_data1],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=source_directory,\n",
    "    allow_reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:22.306978Z",
     "start_time": "2019-12-31T14:38:22.302966Z"
    }
   },
   "outputs": [],
   "source": [
    "source_directory = \"data_dependency_run_extract\"\n",
    "print(f\"Source directory for the step is {os.path.realpath(source_directory)}.\")\n",
    "\n",
    "extractStep = PythonScriptStep(\n",
    "    script_name=\"extract.py\",\n",
    "    arguments=[\"--input_extract\", processed_data1, \"--output_extract\", processed_data2],\n",
    "    inputs=[processed_data1],\n",
    "    outputs=[processed_data2],\n",
    "    compute_target=compute_target, \n",
    "    source_directory=source_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The next step is a bit complex. It consumes intermediate data and existing data, and produces intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:28.165568Z",
     "start_time": "2019-12-31T14:38:28.159575Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineParameter\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "\n",
    "datapath = DataPath(datastore=def_blob_store, path_on_datastore='20newsgroups/20news.pkl')\n",
    "datapath_param = PipelineParameter(name=\"compare_data\", default_value=datapath)\n",
    "data_parameter1 = (datapath_param, DataPathComputeBinding(mode='mount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:30.044005Z",
     "start_time": "2019-12-31T14:38:30.039977Z"
    }
   },
   "outputs": [],
   "source": [
    "source_directory = \"data_dependency_run_compare\"\n",
    "print(f\"Source directory for the step is {os.path.realpath(source_directory)}.\")\n",
    "\n",
    "compareStep = PythonScriptStep(\n",
    "    script_name=\"compare.py\",\n",
    "    arguments=[\"--compare_data1\", data_parameter1, \"--compare_data2\", processed_data2, \"--output_compare\", processed_data3],\n",
    "    inputs=[data_parameter1, processed_data2],\n",
    "    outputs=[processed_data3],    \n",
    "    compute_target=compute_target, \n",
    "    source_directory=source_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:38:35.155769Z",
     "start_time": "2019-12-31T14:38:34.103143Z"
    }
   },
   "outputs": [],
   "source": [
    "# list of steps to run\n",
    "compareModels = [trainStep, extractStep, compareStep]\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[compareModels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T15:33:08.322962Z",
     "start_time": "2019-12-31T15:33:08.310982Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline1.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T15:41:09.460442Z",
     "start_time": "2019-12-31T15:34:30.037293Z"
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Submit the pipeline to be run\n",
    "pipeline_run1 = Experiment(ws, 'Compare_Models_Exp').submit(pipeline1, regenerate_outputs=False)\n",
    "pipeline_run1.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:48:32.814244Z",
     "start_time": "2019-12-31T14:48:25.447284Z"
    }
   },
   "outputs": [],
   "source": [
    "for step in pipeline_run1.get_steps():\n",
    "    print(\"Outputs of step \" + step.name)\n",
    "    \n",
    "    # Get a dictionary of StepRunOutputs with the output name as the key \n",
    "    output_dict = step.get_outputs()\n",
    "    \n",
    "    for name, output in output_dict.items():\n",
    "        \n",
    "        output_reference = output.get_port_data_reference() # Get output port data reference\n",
    "        print(\"\\tname: \" + name)\n",
    "        print(\"\\tdatastore: \" + output_reference.datastore_name)\n",
    "        print(\"\\tpath on datastore: \" + output_reference.path_on_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_run in pipeline_run1.get_children():\n",
    "    status = step_run.get_status()\n",
    "    print('Script:', step_run.name, 'status:', status)\n",
    "    \n",
    "    # Change this if you want to see details even if the Step has succeeded.\n",
    "    if status == \"Failed\":\n",
    "        joblog = step_run.get_job_log()\n",
    "        print('job log:', joblog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download output from the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T14:50:53.660361Z",
     "start_time": "2019-12-31T14:50:46.662907Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('./data/outputs', exist_ok=True)\n",
    "\n",
    "# Retrieve the step runs by name 'train.py'\n",
    "last_step = pipeline_run1.find_step_run('compare.py')\n",
    "\n",
    "if last_step:\n",
    "    last_step_obj = last_step[0] # since we have only one step by name 'train.py'\n",
    "    last_step_obj.get_output_data('processed_data3').download(\"./data/outputs\") # download the output to current directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
